{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a1dfc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d5df3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6db0ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ident = (\n",
    "    \"Stephanie Andrews (jellomoat@gmail.com), \" + \n",
    "    \"scraping for educational purposes\"\n",
    ")\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401f628",
   "metadata": {},
   "source": [
    "## Fetching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "d670cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_url(count, after_id=None):\n",
    "    base_url = \"https://www.reddit.com/subreddits/new\"\n",
    "\n",
    "    # handle if first page\n",
    "    if not after_id:\n",
    "        print(base_url)\n",
    "        return base_url\n",
    "    else:\n",
    "        return f\"{base_url}?count={count}&after=t{after_id}\"\n",
    "\n",
    "def fetch_next_page(page_nr, fetch_limit=25, after_id=None):\n",
    "    count = page_nr * fetch_limit\n",
    "    print(count)\n",
    "    print(after_id)\n",
    "    url = get_next_url(count, after_id)\n",
    "    print(f\"page: {page_nr}\")\n",
    "    print(f\"Fetching {url}\")\n",
    "    try:\n",
    "        return requests.get(\n",
    "            url,\n",
    "            headers=headers\n",
    "        ).text\n",
    "    except:\n",
    "        print(\"No more pages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "36aed56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "None\n",
      "https://www.reddit.com/subreddits/new\n",
      "page: 0\n",
      "Fetching https://www.reddit.com/subreddits/new\n"
     ]
    }
   ],
   "source": [
    "page_nr = 0\n",
    "html = fetch_next_page(page_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "08fccd42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a831dd",
   "metadata": {},
   "source": [
    "## Exploring the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "1773d617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>subreddits</title>"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dom = lxml.html.fromstring(html)\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "67bb79c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r_els = soup.cssselect(\"div > #siteTable\")[0].cssselect(\"div > .subreddit\")\n",
    "# r_els = soup.find(\"div\", id=\"siteTable\").find_all(\"div\", attrs={\"class\": \"subreddit\"})\n",
    "# r_els = soup.select_one(\"div[id=siteTable]\").find_all(\"div\", attrs={\"class\": \"subreddit\"})\n",
    "r_els = soup.select(\"#siteTable > div.subreddit\")\n",
    "\n",
    "len(r_els)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "b26cd0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"thing id-t5_8rf33y odd subreddit\" data-fullname=\"t5_8rf33y\" data-gildings=\"0\" data-type=\"subreddit\" data-whitelist-status=\"\" id=\"thing_t5_8rf33y\" onclick=\"click_thing(this)\"><p class=\"parent\"></p><div class=\"midcol\"><span class=\"fancy-toggle-button subscribe-button toggle\" data-sr_name=\"khyjj6\" style=\"\"><a class=\"option active add login-required\" href=\"#\" tabindex=\"100\">join</a><a class=\"option remove\" href=\"#\">leave</a></span></div><div class=\"entry unvoted\"><p class=\"titlerow\"><a class=\"title\" href=\"https://www.reddit.com/r/khyjj6/\">r/khyjj6: khyjj6</a></p><p class=\"tagline\"><span class=\"score dislikes\" title=\"0\"><span class=\"number\">0</span> <span class=\"word\">subscribers</span></span><span class=\"score unvoted\" title=\"1\"><span class=\"number\">1</span> <span class=\"word\">subscriber</span></span><span class=\"score likes\" title=\"2\"><span class=\"number\">2</span> <span class=\"word\">subscribers</span></span>, a community for 4 minutes</p><ul class=\"flat-list buttons\"><li class=\"report-button login-required\"><a class=\"reportbtn access-required\" data-event-action=\"report\" href=\"javascript:void(0)\">report</a></li></ul><div class=\"reportform report-t5_8rf33y\"></div></div><div class=\"child\"></div><div class=\"clearleft\"></div></div>"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lxml.html.tostring(r_els[0])\n",
    "r_els[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "5e7d876f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joinleaver/khyjj6: khyjj60 subscribers1 subscriber2 subscribers, a community for 4 minutesreport'"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all text for a single subreddit element, incl children\n",
    "# r_els[0].text_content()\n",
    "r_els[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "59ef1329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r/khyjj6: khyjj60 subscribers1 subscriber2 subscribers, a community for 4 minutes',\n",
       " 'r/isabrunellionly: isabrunellionly0 subscribers1 subscriber2 subscribers, a community for 4 minutes',\n",
       " 'r/rivershott: rivershott0 subscribers1 subscriber2 subscribers, a community for 4 minutes']"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all text for all subreddits on page\n",
    "# text_content = [r.text_content().lstrip(\"joinleave\").rstrip(\"report\") for r in r_els]\n",
    "text_content = [r.text.lstrip(\"joinleave\").rstrip(\"report\") for r in r_els]\n",
    "text_content[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18826334",
   "metadata": {},
   "source": [
    "## Parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "2488e66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"thing id-t5_8rf33y odd subreddit\" data-fullname=\"t5_8rf33y\" data-gildings=\"0\" data-type=\"subreddit\" data-whitelist-status=\"\" id=\"thing_t5_8rf33y\" onclick=\"click_thing(this)\"><p class=\"parent\"></p><div class=\"midcol\"><span class=\"fancy-toggle-button subscribe-button toggle\" data-sr_name=\"khyjj6\" style=\"\"><a class=\"option active add login-required\" href=\"#\" tabindex=\"100\">join</a><a class=\"option remove\" href=\"#\">leave</a></span></div><div class=\"entry unvoted\"><p class=\"titlerow\"><a class=\"title\" href=\"https://www.reddit.com/r/khyjj6/\">r/khyjj6: khyjj6</a></p><p class=\"tagline\"><span class=\"score dislikes\" title=\"0\"><span class=\"number\">0</span> <span class=\"word\">subscribers</span></span><span class=\"score unvoted\" title=\"1\"><span class=\"number\">1</span> <span class=\"word\">subscriber</span></span><span class=\"score likes\" title=\"2\"><span class=\"number\">2</span> <span class=\"word\">subscribers</span></span>, a community for 4 minutes</p><ul class=\"flat-list buttons\"><li class=\"report-button login-required\"><a class=\"reportbtn access-required\" data-event-action=\"report\" href=\"javascript:void(0)\">report</a></li></ul><div class=\"reportform report-t5_8rf33y\"></div></div><div class=\"child\"></div><div class=\"clearleft\"></div></div>\n",
      "start count: 0\n",
      "final core_df len: 25\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_all_desc(desc_elements_list):\n",
    "    if len(desc_elements_list) > 0:\n",
    "        return \" \".join([el.text for el in desc_elements_list])\n",
    "    else:\n",
    "        \"\"\n",
    "\n",
    "def parse_and_add_to_df(els_list, start_df):\n",
    "    print(f\"start count: {len(core_df)}\")\n",
    "    search_str = re.compile(r\"a community for (.*)report\")\n",
    "    parsed_subs_list = []\n",
    "\n",
    "    for r in els_list:\n",
    "        sub_data = {\n",
    "        \"name\": r.select_one(\".titlerow\").text.split(\":\")[0],\n",
    "        \"desc\": get_all_desc(r.select(\"div.md > p\")),\n",
    "        \"sub_age_tup\": (re.search(search_str, r.text).group(1).split()),\n",
    "        \"page_id\": r.get(\"id\").lstrip(\"thing_\"),\n",
    "        \"num_subscribers\": r.select_one(\"p.tagline > span.unvoted > span.number\").text,\n",
    "        \"dt_retrieved\": datetime.now()\n",
    "        }\n",
    "        sub_data[\"age_num\"] = sub_data[\"sub_age_tup\"][0]\n",
    "        sub_data[\"age_word\"] = sub_data[\"sub_age_tup\"][1]\n",
    "        sub_data.pop(\"sub_age_tup\")\n",
    "        parsed_subs_list.append(sub_data)\n",
    "\n",
    "#         print(f\"name: {sub_data['name']}\")\n",
    "#         print(f\"age: {sub_data['subs_num']} {sub_data['subs_word']}\")\n",
    "#         print(f\"desc: {sub_data['desc']}\")\n",
    "#         print(sub_data[\"page_id\"])\n",
    "#         print(\"****\")\n",
    "\n",
    "#     print(parsed_subs_list)\n",
    "    end_df = pd.concat([start_df, pd.DataFrame(parsed_subs_list)], ignore_index=True)\n",
    "    print(f\"final core_df len: {len(end_df)}\")\n",
    "    return end_df\n",
    "\n",
    "core_df = pd.DataFrame()\n",
    "print(r_els[0])\n",
    "core_df = parse_and_add_to_df(r_els, core_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "4892b34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "      <th>page_id</th>\n",
       "      <th>num_subscribers</th>\n",
       "      <th>dt_retrieved</th>\n",
       "      <th>age_num</th>\n",
       "      <th>age_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>r/Anabolicminds</td>\n",
       "      <td>Annabolicminds is now live on Reddit</td>\n",
       "      <td>5_8rf22c</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:08:03.664035</td>\n",
       "      <td>14</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>r/kiwihockwypgh</td>\n",
       "      <td>ice and dek hockey vids</td>\n",
       "      <td>5_8rf1vb</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:08:03.670706</td>\n",
       "      <td>15</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>r/LabelBars</td>\n",
       "      <td>Labels you found that could be a rap bar shoul...</td>\n",
       "      <td>5_8rf108</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:08:06.000500</td>\n",
       "      <td>19</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>r/rcwchatsworthga</td>\n",
       "      <td>welcome to the Renegade Championship Wrestling...</td>\n",
       "      <td>5_8rf02e</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:08:06.001800</td>\n",
       "      <td>23</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>r/ReckoningEFed</td>\n",
       "      <td>Home of Reckoning Efed.</td>\n",
       "      <td>5_8rez0a</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:08:06.003417</td>\n",
       "      <td>28</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>r/SWASD2</td>\n",
       "      <td>New official subreddit for the South Williamsp...</td>\n",
       "      <td>5_8reyyg</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-04 23:08:06.003801</td>\n",
       "      <td>28</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>r/hegetsuz</td>\n",
       "      <td>HeGetsUz - submit your memes for HeGetsUz.com</td>\n",
       "      <td>5_8rexwb</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:08:06.006737</td>\n",
       "      <td>32</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>r/Taliyaandgustavo_Pr</td>\n",
       "      <td>wellcome to https://newporntv.me/</td>\n",
       "      <td>5_8rewtt</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-04 23:08:08.491291</td>\n",
       "      <td>37</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>r/DragonPuppetsClub</td>\n",
       "      <td>This community is for the people who love drag...</td>\n",
       "      <td>5_8rewq5</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:08:08.491794</td>\n",
       "      <td>38</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>r/TickleLand</td>\n",
       "      <td>tigglemytoes community</td>\n",
       "      <td>5_8rew5j</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:08:08.492851</td>\n",
       "      <td>40</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>r/batalha_machine</td>\n",
       "      <td>bem vindo(a) a comunidade oficial da batalha m...</td>\n",
       "      <td>5_8rev0h</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:08:08.494806</td>\n",
       "      <td>45</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>r/SnarkShark</td>\n",
       "      <td>We're here to shed light on coy shark ladies i...</td>\n",
       "      <td>5_8reuwd</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-04 23:08:08.495287</td>\n",
       "      <td>46</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>r/LeaseswapAuto</td>\n",
       "      <td>A community connect auto lease enthusiasts for...</td>\n",
       "      <td>5_8reul8</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:08:08.496473</td>\n",
       "      <td>47</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>r/BleachSRgame</td>\n",
       "      <td>Unofficial subreddit for the brand-new 3D swor...</td>\n",
       "      <td>5_8retvt</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-04 23:08:08.496901</td>\n",
       "      <td>50</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                                               desc  \\\n",
       "18        r/Anabolicminds               Annabolicminds is now live on Reddit   \n",
       "20        r/kiwihockwypgh                            ice and dek hockey vids   \n",
       "28            r/LabelBars  Labels you found that could be a rap bar shoul...   \n",
       "31      r/rcwchatsworthga  welcome to the Renegade Championship Wrestling...   \n",
       "35        r/ReckoningEFed                           Home of Reckoning Efed.    \n",
       "36               r/SWASD2  New official subreddit for the South Williamsp...   \n",
       "43             r/hegetsuz      HeGetsUz - submit your memes for HeGetsUz.com   \n",
       "50  r/Taliyaandgustavo_Pr                  wellcome to https://newporntv.me/   \n",
       "51    r/DragonPuppetsClub  This community is for the people who love drag...   \n",
       "53           r/TickleLand                             tigglemytoes community   \n",
       "58      r/batalha_machine  bem vindo(a) a comunidade oficial da batalha m...   \n",
       "59           r/SnarkShark  We're here to shed light on coy shark ladies i...   \n",
       "62        r/LeaseswapAuto  A community connect auto lease enthusiasts for...   \n",
       "63         r/BleachSRgame  Unofficial subreddit for the brand-new 3D swor...   \n",
       "\n",
       "     page_id num_subscribers               dt_retrieved age_num age_word  \n",
       "18  5_8rf22c               1 2023-07-04 23:08:03.664035      14  minutes  \n",
       "20  5_8rf1vb               1 2023-07-04 23:08:03.670706      15  minutes  \n",
       "28  5_8rf108               1 2023-07-04 23:08:06.000500      19  minutes  \n",
       "31  5_8rf02e               1 2023-07-04 23:08:06.001800      23  minutes  \n",
       "35  5_8rez0a               1 2023-07-04 23:08:06.003417      28  minutes  \n",
       "36  5_8reyyg               2 2023-07-04 23:08:06.003801      28  minutes  \n",
       "43  5_8rexwb               1 2023-07-04 23:08:06.006737      32  minutes  \n",
       "50  5_8rewtt               2 2023-07-04 23:08:08.491291      37  minutes  \n",
       "51  5_8rewq5               1 2023-07-04 23:08:08.491794      38  minutes  \n",
       "53  5_8rew5j               1 2023-07-04 23:08:08.492851      40  minutes  \n",
       "58  5_8rev0h               1 2023-07-04 23:08:08.494806      45  minutes  \n",
       "59  5_8reuwd               2 2023-07-04 23:08:08.495287      46  minutes  \n",
       "62  5_8reul8               1 2023-07-04 23:08:08.496473      47  minutes  \n",
       "63  5_8retvt               2 2023-07-04 23:08:08.496901      50  minutes  "
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of records with desc\n",
    "core_df[~core_df[\"desc\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "53248b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_word\n",
       "minutes    75\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_df[\"age_word\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581827b",
   "metadata": {},
   "source": [
    "## Get next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "0d83679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1\n",
      "Fetching page 2\n",
      "Fetching page 3\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Fetching page \" + str(i + 1))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "10e2860e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.reddit.com/subreddits/new?count=100&after=t5_8r8ktl'"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_url(100, \"5_8r8ktl\")\n",
    "# https://www.reddit.com/subreddits/new?count=25&after=t5_8r8ktl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "87689736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74    5_8resct\n",
       "Name: page_id, dtype: object"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_df.loc[core_df[\"dt_retrieved\"] == core_df[\"dt_retrieved\"].max()][\"page_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "47979eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5_8rescs'"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_page_id = core_df.loc[core_df[\"dt_retrieved\"] == core_df[\"dt_retrieved\"].max()][\"page_id\"].values[0] \\\n",
    "    if (len(core_df) > 1) else None\n",
    "last_page_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "13646072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last page id => None\n",
      "0\n",
      "None\n",
      "https://www.reddit.com/subreddits/new\n",
      "page: 0\n",
      "Fetching https://www.reddit.com/subreddits/new\n",
      "<title>subreddits</title>\n",
      "start count: 0\n",
      "final core_df len: 25\n",
      "last page id => 5_8rf1a4\n",
      "25\n",
      "5_8rf1a4\n",
      "page: 1\n",
      "Fetching https://www.reddit.com/subreddits/new?count=25&after=t5_8rf1a4\n",
      "<title>subreddits</title>\n",
      "start count: 25\n",
      "final core_df len: 50\n",
      "last page id => 5_8rewyo\n",
      "50\n",
      "5_8rewyo\n",
      "page: 2\n",
      "Fetching https://www.reddit.com/subreddits/new?count=50&after=t5_8rewyo\n",
      "<title>subreddits</title>\n",
      "start count: 50\n",
      "final core_df len: 75\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "core_df = pd.DataFrame()\n",
    "result_limit = 25\n",
    "\n",
    "# fetch each page\n",
    "# if last resultset returned less than the limit, stop fetching\n",
    "for page_nr in range(3): # swap to while loop after\n",
    "    last_page_id = core_df.loc[core_df[\"dt_retrieved\"] == core_df[\"dt_retrieved\"].max()][\"page_id\"].values[0] \\\n",
    "        if (len(core_df) > 1) else None\n",
    "    print(f\"last page id => {last_page_id}\")\n",
    "    html = fetch_next_page(page_nr, result_limit, last_page_id)\n",
    "    sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    print(soup.title)\n",
    "\n",
    "    # parse each page, add to df\n",
    "    r_els = soup.select(\"#siteTable > div.subreddit\")\n",
    "    core_df = parse_and_add_to_df(r_els, core_df)\n",
    "    if len(r_els) < 25:\n",
    "        print(len(r_els))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "a6c9c5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "page_id\n",
       "5_8rf5od    1\n",
       "5_8revg0    1\n",
       "5_8rew5j    1\n",
       "5_8rew9p    1\n",
       "5_8rewq5    1\n",
       "           ..\n",
       "5_8rf1a4    1\n",
       "5_8rf1dq    1\n",
       "5_8rf1tg    1\n",
       "5_8rf1vb    1\n",
       "5_8resct    1\n",
       "Name: count, Length: 75, dtype: int64"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_df[\"page_id\"].value_counts()\n",
    "# FIX: LAST_PAGE_ID IS NOT CHANGING WHAT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "e6ff3365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "      <th>page_id</th>\n",
       "      <th>num_subscribers</th>\n",
       "      <th>dt_retrieved</th>\n",
       "      <th>age_num</th>\n",
       "      <th>age_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>r/Anabolicminds</td>\n",
       "      <td>Annabolicminds is now live on Reddit</td>\n",
       "      <td>5_8rf22c</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:09:53.816635</td>\n",
       "      <td>16</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>r/kiwihockwypgh</td>\n",
       "      <td>ice and dek hockey vids</td>\n",
       "      <td>5_8rf1vb</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:09:53.817436</td>\n",
       "      <td>17</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>r/LabelBars</td>\n",
       "      <td>Labels you found that could be a rap bar shoul...</td>\n",
       "      <td>5_8rf108</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:09:56.189381</td>\n",
       "      <td>21</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>r/rcwchatsworthga</td>\n",
       "      <td>welcome to the Renegade Championship Wrestling...</td>\n",
       "      <td>5_8rf02e</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:09:56.190518</td>\n",
       "      <td>25</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>r/ReckoningEFed</td>\n",
       "      <td>Home of Reckoning Efed.</td>\n",
       "      <td>5_8rez0a</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:09:56.192022</td>\n",
       "      <td>29</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>r/SWASD2</td>\n",
       "      <td>New official subreddit for the South Williamsp...</td>\n",
       "      <td>5_8reyyg</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-04 23:09:56.192373</td>\n",
       "      <td>30</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>r/hegetsuz</td>\n",
       "      <td>HeGetsUz - submit your memes for HeGetsUz.com</td>\n",
       "      <td>5_8rexwb</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:09:56.194912</td>\n",
       "      <td>34</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>r/Taliyaandgustavo_Pr</td>\n",
       "      <td>wellcome to https://newporntv.me/</td>\n",
       "      <td>5_8rewtt</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-04 23:09:58.701896</td>\n",
       "      <td>39</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>r/DragonPuppetsClub</td>\n",
       "      <td>This community is for the people who love drag...</td>\n",
       "      <td>5_8rewq5</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:09:58.702402</td>\n",
       "      <td>40</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>r/TickleLand</td>\n",
       "      <td>tigglemytoes community</td>\n",
       "      <td>5_8rew5j</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:09:58.703250</td>\n",
       "      <td>42</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>r/batalha_machine</td>\n",
       "      <td>bem vindo(a) a comunidade oficial da batalha m...</td>\n",
       "      <td>5_8rev0h</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:09:58.705246</td>\n",
       "      <td>47</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>r/SnarkShark</td>\n",
       "      <td>We're here to shed light on coy shark ladies i...</td>\n",
       "      <td>5_8reuwd</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-04 23:09:58.705689</td>\n",
       "      <td>48</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>r/LeaseswapAuto</td>\n",
       "      <td>A community connect auto lease enthusiasts for...</td>\n",
       "      <td>5_8reul8</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-04 23:09:58.706940</td>\n",
       "      <td>49</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>r/BleachSRgame</td>\n",
       "      <td>Unofficial subreddit for the brand-new 3D swor...</td>\n",
       "      <td>5_8retvt</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-04 23:09:58.707330</td>\n",
       "      <td>52</td>\n",
       "      <td>minutes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                                               desc  \\\n",
       "19        r/Anabolicminds               Annabolicminds is now live on Reddit   \n",
       "21        r/kiwihockwypgh                            ice and dek hockey vids   \n",
       "29            r/LabelBars  Labels you found that could be a rap bar shoul...   \n",
       "32      r/rcwchatsworthga  welcome to the Renegade Championship Wrestling...   \n",
       "36        r/ReckoningEFed                           Home of Reckoning Efed.    \n",
       "37               r/SWASD2  New official subreddit for the South Williamsp...   \n",
       "44             r/hegetsuz      HeGetsUz - submit your memes for HeGetsUz.com   \n",
       "51  r/Taliyaandgustavo_Pr                  wellcome to https://newporntv.me/   \n",
       "52    r/DragonPuppetsClub  This community is for the people who love drag...   \n",
       "54           r/TickleLand                             tigglemytoes community   \n",
       "59      r/batalha_machine  bem vindo(a) a comunidade oficial da batalha m...   \n",
       "60           r/SnarkShark  We're here to shed light on coy shark ladies i...   \n",
       "63        r/LeaseswapAuto  A community connect auto lease enthusiasts for...   \n",
       "64         r/BleachSRgame  Unofficial subreddit for the brand-new 3D swor...   \n",
       "\n",
       "     page_id num_subscribers               dt_retrieved age_num age_word  \n",
       "19  5_8rf22c               1 2023-07-04 23:09:53.816635      16  minutes  \n",
       "21  5_8rf1vb               1 2023-07-04 23:09:53.817436      17  minutes  \n",
       "29  5_8rf108               1 2023-07-04 23:09:56.189381      21  minutes  \n",
       "32  5_8rf02e               1 2023-07-04 23:09:56.190518      25  minutes  \n",
       "36  5_8rez0a               1 2023-07-04 23:09:56.192022      29  minutes  \n",
       "37  5_8reyyg               2 2023-07-04 23:09:56.192373      30  minutes  \n",
       "44  5_8rexwb               1 2023-07-04 23:09:56.194912      34  minutes  \n",
       "51  5_8rewtt               2 2023-07-04 23:09:58.701896      39  minutes  \n",
       "52  5_8rewq5               1 2023-07-04 23:09:58.702402      40  minutes  \n",
       "54  5_8rew5j               1 2023-07-04 23:09:58.703250      42  minutes  \n",
       "59  5_8rev0h               1 2023-07-04 23:09:58.705246      47  minutes  \n",
       "60  5_8reuwd               2 2023-07-04 23:09:58.705689      48  minutes  \n",
       "63  5_8reul8               1 2023-07-04 23:09:58.706940      49  minutes  \n",
       "64  5_8retvt               2 2023-07-04 23:09:58.707330      52  minutes  "
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_df[~core_df[\"desc\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd9386",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "408ac055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log exists\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "pages_dir = \"raw-pages/\"\n",
    "\n",
    "# setup\n",
    "core_df = pd.DataFrame()\n",
    "result_limit = 25\n",
    "\n",
    "# create or get fetch log\n",
    "log = Path(pages_dir + \"log.csv\")\n",
    "\n",
    "if log.exists():\n",
    "    print(\"log exists\")\n",
    "else:\n",
    "    with open(log, \"w\") as l:\n",
    "        print(\"test\")\n",
    "        l.write(\"fetch_dt, url, page_nr\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "31f09754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already have raw-pages/0.html, loading!\n",
      "<title>subreddits</title>\n",
      "start count: 0\n",
      "final core_df len: 25\n",
      "Already have raw-pages/1.html, loading!\n",
      "<title>subreddits</title>\n",
      "start count: 25\n",
      "final core_df len: 50\n",
      "Already have raw-pages/2.html, loading!\n",
      "<title>subreddits</title>\n",
      "start count: 50\n",
      "final core_df len: 75\n"
     ]
    }
   ],
   "source": [
    "# fetch each page\n",
    "# if last resultset returned less than the limit (100), stop fetching\n",
    "for page_nr in range(3): # swap to while loop after\n",
    "    dest = Path(pages_dir + str(page_nr) + \".html\")\n",
    "    \n",
    "    if dest.exists(): # load it from file\n",
    "        print(f\"Already have {dest}, loading!\")\n",
    "        file = open(dest, \"r\")\n",
    "        page_html = file.read()\n",
    "        file.close()\n",
    "\n",
    "    else: # fetch it!\n",
    "        last_page_id = core_df.loc[core_df[\"dt_retrieved\"] == core_df[\"dt_retrieved\"].max()][\"page_id\"].values[0] \\\n",
    "            if (len(core_df) > 1) else None\n",
    "        print(f\"page #{page_nr}: last page id => {last_page_id}\")\n",
    "        page_html = fetch_next_page(page_nr, 25, last_page_id)\n",
    "        \n",
    "        # save to file\n",
    "        with open(dest, \"w\") as f:\n",
    "            f.write(page_html)\n",
    "\n",
    "        with open(log, \"a\") as l:\n",
    "            fetch_dt = datetime.now()\n",
    "            url = get_next_url(page_nr * 25, last_page_id)\n",
    "            l.write(\",\".join([str(fetch_dt), url, str(page_nr)]) + \"\\n\")\n",
    "\n",
    "    soup = BeautifulSoup(page_html, \"lxml\")\n",
    "    print(soup.title)\n",
    "\n",
    "    # parse each page, add to df\n",
    "    r_els = soup.select(\"#siteTable > div.subreddit\")\n",
    "    core_df = parse_and_add_to_df(r_els, core_df)\n",
    "    sleep(2)\n",
    "    if len(r_els) < 25:\n",
    "        print(len(r_els))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "af1a7419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-07-04 23:39:37.885966')"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_df[\"dt_retrieved\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "dc8918b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74    5_8reztl\n",
       "Name: page_id, dtype: object"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_df.loc[core_df[\"dt_retrieved\"] == core_df[\"dt_retrieved\"].max()][\"page_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed8463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
